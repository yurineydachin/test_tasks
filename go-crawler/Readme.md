Написать веб-краулер.
Язык реализации: golang.
Краулер должен быть многопоточным (что такое многопоточный, специально не оговаривается, но выбранную схему нужно будет защитить в обсуждение, почему вот это мы параллелим, вот это нет, почему параллелим так, а не эдак)
Краулер скачивает только текстовые документы (как именно их отличать - весьма многогранный © вопрос. Я могу придумать минимум два способа это делать, со своими плюсами и минусами, решение нужно будет защитить).
Краулер должен сохранять документы на диск.
Краулер должен поддерживать докачку (понимать, что уже скачано, что нет, продолжать закачку места остановки)
На вход приходит url вида http(s)://domain/path
Краулер идет вглубь только по данному url + суффисы.
Лучше пояснить на пример, что подходит:
- http(s)://domain/path
- http(s)://domain/path/
- http(s)://domain/path/a
- http(s)://domain/path/a/c
Не подходит
- http(s)://foo.domain/path
- http(s)://domain/bar
Интерфейс: command-line
Краулер должен обрабатывать Ctrl+C и корректно при этом завершаться
